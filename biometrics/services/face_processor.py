import base64
import io
import logging
import time
from typing import Any, Dict, List, Optional, Tuple

import cv2
import face_recognition
import numpy as np
from PIL import Image

from django.conf import settings

logger = logging.getLogger(__name__)


class FaceProcessor:
    """Service for processing face images and extracting embeddings"""

    def __init__(self):
        self.tolerance = getattr(settings, "FACE_RECOGNITION_TOLERANCE", 0.4)
        self.model = getattr(settings, "FACE_ENCODING_MODEL", "large")
        self.min_face_size = getattr(settings, "MIN_FACE_SIZE", (50, 50))
        self.quality_threshold = getattr(settings, "FACE_QUALITY_THRESHOLD", 0.65)

    def decode_base64_image(self, base64_string: str) -> Optional[np.ndarray]:
        """
        Enhanced base64 image decoder with validation and error handling

        Args:
            base64_string: Base64 encoded image

        Returns:
            Numpy array of the image or None if failed
        """
        try:
            # Input validation
            if not base64_string or not isinstance(base64_string, str):
                logger.error("Invalid base64 string: empty or not string")
                return None

            # Remove data URI prefix if present
            if "," in base64_string:
                parts = base64_string.split(",", 1)
                if len(parts) > 1:
                    base64_string = parts[1]
                    logger.debug(
                        f"Removed data URI prefix, image data length: {len(base64_string)}"
                    )

            # Validate base64 string length
            if len(base64_string) < 100:  # Too short for a real image
                logger.error(
                    f"Base64 string too short: {len(base64_string)} characters"
                )
                return None

            # Clean base64 string (remove whitespace)
            base64_string = base64_string.strip().replace("\n", "").replace("\r", "")

            # Validate base64 format
            import base64

            try:
                # Test decode small portion first
                test_decode = base64.b64decode(base64_string[:100])
                logger.debug(
                    f"Base64 validation passed, test decode length: {len(test_decode)}"
                )
            except Exception as e:
                from core.logging_utils import err_tag
                logger.error("Invalid base64 format", extra={"err": err_tag(e)})
                return None

            # Decode full base64
            image_data = base64.b64decode(base64_string)
            logger.info(f"Successfully decoded base64 to {len(image_data)} bytes")

            # Validate image data size
            if len(image_data) < 1000:  # Too small for a real image
                logger.error(f"Decoded image data too small: {len(image_data)} bytes")
                return None

            # Convert to PIL Image with better error handling
            try:
                image_buffer = io.BytesIO(image_data)
                image = Image.open(image_buffer)

                # Validate image dimensions
                width, height = image.size
                logger.info(f"Image loaded: {width}x{height}, mode: {image.mode}")

                if width < 50 or height < 50:
                    logger.error(f"Image too small: {width}x{height}")
                    return None

                if width > 4000 or height > 4000:
                    logger.warning(
                        f"Very large image: {width}x{height}, consider resizing"
                    )

                # Convert to RGB if necessary
                if image.mode != "RGB":
                    logger.debug(f"Converting image from {image.mode} to RGB")
                    image = image.convert("RGB")

                # Convert to numpy array
                image_array = np.array(image)
                logger.info(
                    f"Successfully converted to numpy array: {image_array.shape}"
                )

                return image_array

            except Exception as pil_error:
                logger.error(f"PIL Image processing failed: {pil_error}")
                return None

        except Exception as e:
            from core.logging_utils import err_tag
            logger.error("Failed to decode base64 image", extra={"err": err_tag(e)})
            logger.debug(f"Base64 string preview: {base64_string[:100]}...")
            return None

    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocess image for better face detection

        Args:
            image: Input image array

        Returns:
            Preprocessed image array
        """
        try:
            logger.info(f"Preprocessing image: {image.shape}")

            # Resize if too large (for performance)
            height, width = image.shape[:2]
            max_dimension = 1024

            if max(height, width) > max_dimension:
                scale = max_dimension / max(height, width)
                new_width = int(width * scale)
                new_height = int(height * scale)
                image = cv2.resize(
                    image, (new_width, new_height), interpolation=cv2.INTER_AREA
                )
                logger.info(f"Resized image to: {image.shape}")

            # Auto-adjust brightness and contrast
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            l_channel, a, b = cv2.split(lab)

            # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            l_channel = clahe.apply(l_channel)

            # Merge channels and convert back to RGB
            lab = cv2.merge((l_channel, a, b))
            enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)

            logger.info("Applied CLAHE enhancement")

            # Slight noise reduction
            enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
            logger.info("Applied bilateral filter for noise reduction")

            return enhanced

        except Exception as e:
            from core.logging_utils import err_tag
            logger.error("Image preprocessing failed", extra={"err": err_tag(e)})
            return image  # Return original if preprocessing fails

    def check_image_quality(self, image: np.ndarray) -> Dict[str, Any]:
        """
        Check image quality metrics

        Args:
            image: Numpy array of the image

        Returns:
            Dictionary with quality metrics
        """
        try:
            # Convert to grayscale for analysis
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

            # Check brightness
            brightness = np.mean(gray)
            is_too_dark = brightness < 50
            is_too_bright = brightness > 180  # Lowered threshold for test compatibility

            # Check blur using Laplacian variance
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            is_blurry = laplacian_var < 25  # Relaxed blur detection for demo stability

            # Calculate overall quality score
            quality_score = 1.0
            if is_too_dark or is_too_bright:
                quality_score *= 0.5  # More aggressive penalty for brightness issues
            if is_blurry:
                quality_score *= 0.8

            return {
                "brightness": float(brightness),
                "blur_score": float(laplacian_var),
                "is_too_dark": is_too_dark,
                "is_too_bright": is_too_bright,
                "is_blurry": is_blurry,
                "quality_score": quality_score,
                "passed": quality_score >= self.quality_threshold,
            }

        except Exception as e:
            logger.exception("Failed to check image quality")
            return {
                "quality_score": 0,
                "passed": False,
                "error": "Image quality check failed",
            }

    def detect_faces(self, image: np.ndarray) -> Tuple[List, List]:
        """
        Detect faces in the image with aggressive detection methods

        Args:
            image: Numpy array of the image

        Returns:
            Tuple of (face_locations, face_landmarks)
        """
        try:
            detection_start_time = time.time()
            original_shape = image.shape
            logger.info(f"üïê Starting face detection on image: {original_shape}")

            # DEBUG: Check image properties
            logger.info(f"üîç Image analysis:")
            logger.info(f"   - Shape: {image.shape}")
            logger.info(f"   - Data type: {image.dtype}")
            logger.info(f"   - Min/Max values: {image.min()}/{image.max()}")
            logger.info(f"   - Mean brightness: {np.mean(image):.2f}")

            # Try on multiple image sizes for better detection
            working_image = image.copy()
            face_locations = []
            scale_factor = 1.0

            # Method 0: FAST detection first - prioritize speed over accuracy
            method_start_time = time.time()
            logger.info("üöÄ Trying FAST detection with minimal processing...")
            try:
                # Try HOG first with no upsampling (fastest)
                hog_start = time.time()
                face_locations = face_recognition.face_locations(
                    working_image, number_of_times_to_upsample=0, model="hog"
                )
                hog_time = (time.time() - hog_start) * 1000
                logger.info(
                    f"‚ö° Fast HOG (no upsample): {len(face_locations)} faces in {hog_time:.0f}ms"
                )

                # Only try one upsampling if nothing found
                if not face_locations:
                    hog_start = time.time()
                    face_locations = face_recognition.face_locations(
                        working_image, number_of_times_to_upsample=1, model="hog"
                    )
                    hog_time = (time.time() - hog_start) * 1000
                    logger.info(
                        f"‚ö° Fast HOG (upsample=1): {len(face_locations)} faces in {hog_time:.0f}ms"
                    )

            except Exception as e:
                logger.warning(f"Fast detection failed: {e}")

            method_time = (time.time() - method_start_time) * 1000
            logger.info(f"‚è±Ô∏è Method 0 total time: {method_time:.0f}ms")

            # Method 1: Try original size first (if not too large)
            if not face_locations and max(image.shape[:2]) <= 800:
                logger.info("Trying original size detection...")
                try:
                    face_locations = face_recognition.face_locations(
                        working_image, model="hog"
                    )
                    logger.info(f"Original size HOG: {len(face_locations)} faces")
                    if not face_locations:
                        face_locations = face_recognition.face_locations(
                            working_image, model="cnn"
                        )
                        logger.info(f"Original size CNN: {len(face_locations)} faces")
                except Exception as e:
                    logger.warning(f"Original size detection failed: {e}")

            # Method 2: Resize to optimal size (800px max)
            if not face_locations:
                target_size = 800
                if max(image.shape[:2]) > target_size:
                    scale_factor = target_size / max(image.shape[:2])
                    new_width = int(image.shape[1] * scale_factor)
                    new_height = int(image.shape[0] * scale_factor)
                    working_image = cv2.resize(image, (new_width, new_height))
                    logger.info(f"Resized to {working_image.shape} for detection")

                logger.info("Trying resized image HOG detection...")
                try:
                    face_locations = face_recognition.face_locations(
                        working_image, model="hog"
                    )
                    logger.info(f"Resized HOG: {len(face_locations)} faces")
                except Exception as e:
                    logger.warning(f"Resized HOG failed: {e}")

            # Method 3: Try upsampling on resized image
            if not face_locations:
                logger.info("Trying upsampled detection...")
                try:
                    face_locations = face_recognition.face_locations(
                        working_image, number_of_times_to_upsample=1
                    )
                    logger.info(f"Upsampled: {len(face_locations)} faces")
                except Exception as e:
                    logger.warning(f"Upsampled detection failed: {e}")

            # Method 4: Skip CNN entirely for speed - move to method 7

            # Method 5: OpenCV cascade as fallback - MORE AGGRESSIVE
            if not face_locations:
                logger.info("Trying OpenCV Haar cascades...")
                try:
                    gray = cv2.cvtColor(working_image, cv2.COLOR_RGB2GRAY)

                    # Try different cascade files
                    cascade_files = [
                        "haarcascade_frontalface_default.xml",
                        "haarcascade_frontalface_alt.xml",
                        "haarcascade_frontalface_alt2.xml",
                    ]

                    for cascade_file in cascade_files:
                        try:
                            cascade_path = cv2.data.haarcascades + cascade_file
                            face_cascade = cv2.CascadeClassifier(cascade_path)

                            # Try different scale factors AND minimum neighbors
                            for scale in [1.05, 1.1, 1.15, 1.2, 1.3, 1.5]:
                                for min_neighbors in [1, 2, 3, 4, 5]:
                                    faces = face_cascade.detectMultiScale(
                                        gray, scale, min_neighbors
                                    )
                                    if len(faces) > 0:
                                        face_locations = [
                                            (y, x + w, y + h, x)
                                            for (x, y, w, h) in faces
                                        ]
                                        logger.info(
                                            f"OpenCV {cascade_file} scale {scale} neighbors {min_neighbors}: {len(face_locations)} faces"
                                        )
                                        break
                                if face_locations:
                                    break
                            if face_locations:
                                break
                        except Exception as e:
                            logger.warning(f"OpenCV cascade {cascade_file} failed: {e}")
                except Exception as e:
                    logger.warning(f"All OpenCV detection failed: {e}")

            # Method 6: Try with image enhancement - MULTIPLE ENHANCEMENTS
            if not face_locations:
                logger.info("Trying with enhanced image...")
                try:
                    # Try different enhancement techniques
                    enhancements = [
                        # Contrast and brightness
                        {"alpha": 1.2, "beta": 30, "name": "contrast_bright"},
                        {"alpha": 1.5, "beta": 20, "name": "high_contrast"},
                        {"alpha": 0.8, "beta": 40, "name": "low_contrast_bright"},
                        {"alpha": 1.0, "beta": 50, "name": "brightness_only"},
                    ]

                    for enhancement in enhancements:
                        enhanced = cv2.convertScaleAbs(
                            working_image,
                            alpha=enhancement["alpha"],
                            beta=enhancement["beta"],
                        )

                        # Try both HOG and OpenCV on enhanced image
                        face_locations = face_recognition.face_locations(
                            enhanced, model="hog"
                        )
                        if face_locations:
                            logger.info(
                                f"Enhanced image {enhancement['name']} HOG: {len(face_locations)} faces"
                            )
                            working_image = enhanced
                            break

                        # Try OpenCV on enhanced image
                        gray_enhanced = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
                        face_cascade = cv2.CascadeClassifier(
                            cv2.data.haarcascades
                            + "haarcascade_frontalface_default.xml"
                        )
                        faces = face_cascade.detectMultiScale(gray_enhanced, 1.1, 2)
                        if len(faces) > 0:
                            face_locations = [
                                (y, x + w, y + h, x) for (x, y, w, h) in faces
                            ]
                            logger.info(
                                f"Enhanced image {enhancement['name']} OpenCV: {len(face_locations)} faces"
                            )
                            working_image = enhanced
                            break

                except Exception as e:
                    logger.warning(f"Enhanced detection failed: {e}")

            # Method 7: Histogram equalization
            if not face_locations:
                logger.info("Trying histogram equalization...")
                try:
                    gray = cv2.cvtColor(working_image, cv2.COLOR_RGB2GRAY)
                    equalized = cv2.equalizeHist(gray)
                    equalized_rgb = cv2.cvtColor(equalized, cv2.COLOR_GRAY2RGB)

                    face_locations = face_recognition.face_locations(
                        equalized_rgb, model="hog"
                    )
                    if face_locations:
                        logger.info(f"Histogram equalized: {len(face_locations)} faces")
                        working_image = equalized_rgb
                    else:
                        # Try OpenCV on histogram equalized
                        face_cascade = cv2.CascadeClassifier(
                            cv2.data.haarcascades
                            + "haarcascade_frontalface_default.xml"
                        )
                        faces = face_cascade.detectMultiScale(equalized, 1.1, 2)
                        if len(faces) > 0:
                            face_locations = [
                                (y, x + w, y + h, x) for (x, y, w, h) in faces
                            ]
                            logger.info(
                                f"Histogram equalized OpenCV: {len(face_locations)} faces"
                            )
                            working_image = equalized_rgb
                except Exception as e:
                    logger.warning(f"Histogram equalization failed: {e}")

            # Method 8: CNN detection as LAST RESORT on very small image
            if not face_locations:
                logger.info("Trying CNN detection as last resort...")
                try:
                    # Make image very small for CNN (200px max)
                    if max(working_image.shape[:2]) > 200:
                        cnn_scale = 200 / max(working_image.shape[:2])
                        cnn_width = int(working_image.shape[1] * cnn_scale)
                        cnn_height = int(working_image.shape[0] * cnn_scale)
                        cnn_image = cv2.resize(working_image, (cnn_width, cnn_height))
                        scale_factor *= cnn_scale
                    else:
                        cnn_image = working_image

                    face_locations = face_recognition.face_locations(
                        cnn_image, model="cnn"
                    )
                    logger.info(f"CNN (last resort): {len(face_locations)} faces")
                    working_image = cnn_image  # Update working image for scaling
                except Exception as e:
                    logger.warning(f"CNN detection failed: {e}")

            # Scale face_locations back to original size
            if face_locations and scale_factor != 1.0:
                scale_back = 1.0 / scale_factor
                scaled_locations = []
                for top, right, bottom, left in face_locations:
                    scaled_locations.append(
                        (
                            int(top * scale_back),
                            int(right * scale_back),
                            int(bottom * scale_back),
                            int(left * scale_back),
                        )
                    )
                face_locations = scaled_locations
                logger.info(f"Scaled face locations back by factor {scale_back}")

            # Get face landmarks if faces found
            face_landmarks = []
            if face_locations:
                try:
                    # Use the image coordinates that match the face_locations
                    # face_locations are already in working_image coordinates after scaling
                    face_landmarks = face_recognition.face_landmarks(
                        working_image, face_locations
                    )
                    logger.info(
                        f"Successfully extracted landmarks for {len(face_landmarks)} faces"
                    )

                    # Debug: check if landmarks contain eyes
                    for i, landmarks in enumerate(face_landmarks):
                        landmark_keys = list(landmarks.keys())
                        has_eyes = (
                            "left_eye" in landmark_keys and "right_eye" in landmark_keys
                        )
                        logger.info(
                            f"Face {i+1} landmarks: {landmark_keys}, has_eyes: {has_eyes}"
                        )

                except Exception as e:
                    logger.warning(f"Failed to get face landmarks: {e}")
                    face_landmarks = []

            total_detection_time = (time.time() - detection_start_time) * 1000
            logger.info(
                f"üèÅ FINAL DETECTION RESULT: {len(face_locations)} faces found in {total_detection_time:.0f}ms"
            )
            if face_locations:
                for i, (top, right, bottom, left) in enumerate(face_locations):
                    face_width = right - left
                    face_height = bottom - top
                    logger.info(
                        f"Face {i+1}: size {face_width}x{face_height}, position ({left},{top})"
                    )

            return face_locations, face_landmarks

        except Exception as e:
            logger.exception("Face detection completely failed")
            return [], []

    def extract_face_encoding(
        self, image: np.ndarray, face_location: Tuple, face_landmarks: List = None
    ) -> Tuple[Optional[np.ndarray], bool]:
        """
        Extract face encoding from a specific face location and check eye visibility

        Args:
            image: Numpy array of the image
            face_location: Tuple of (top, right, bottom, left)
            face_landmarks: Optional list of face landmarks

        Returns:
            Tuple of (128-dimensional face encoding or None, has_eyes boolean)
        """
        try:
            # Extract face encoding
            encodings = face_recognition.face_encodings(
                image, known_face_locations=[face_location], model=self.model
            )

            # Check eye visibility
            has_eyes = False
            if face_landmarks:
                has_eyes = (
                    "left_eye" in face_landmarks and "right_eye" in face_landmarks
                )
                logger.info(
                    f"üëÅÔ∏è Eye visibility check in extract_face_encoding: has_eyes={has_eyes}"
                )
                if face_landmarks:
                    landmarks_keys = list(face_landmarks.keys())
                    logger.info(f"üëÅÔ∏è Available landmarks: {landmarks_keys}")
            else:
                logger.warning("üëÅÔ∏è No face_landmarks provided to extract_face_encoding")

            if encodings:
                return encodings[0], has_eyes
            return None, has_eyes

        except Exception as e:
            logger.exception("Failed to extract face encoding")
            return None, False

    def process_registration_image(self, base64_image: str) -> Dict[str, Any]:
        """
        Process a single image for registration

        Args:
            base64_image: Base64 encoded image

        Returns:
            Dictionary with processing results
        """
        start_time = time.time()
        logger.info("Starting biometric image processing")

        # Decode image
        logger.info(f"Decoding base64 image (length: {len(base64_image)})")
        image = self.decode_base64_image(base64_image)
        if image is None:
            logger.error("Failed to decode base64 image")
            return {"success": False, "error": "Failed to decode image"}

        logger.info(f"Image decoded successfully: shape={image.shape}")

        # Preprocess image for better detection
        logger.info("Preprocessing image for optimal face detection")
        processed_image = self.preprocess_image(image)
        logger.info(f"Image preprocessing completed: {processed_image.shape}")

        # Check image quality on processed image
        logger.info("Checking processed image quality")
        quality_check = self.check_image_quality(processed_image)
        logger.info(f"Quality check result: {quality_check}")

        # More lenient quality check for real biometric processing
        quality_threshold = 0.3  # Lower threshold for real images
        if quality_check["quality_score"] < quality_threshold:
            logger.warning(
                f"Image quality below threshold ({quality_threshold}): {quality_check['quality_score']}"
            )
            # Try to enhance the image further
            if quality_check["is_too_dark"]:
                logger.info("Attempting to brighten dark image")
                processed_image = cv2.convertScaleAbs(
                    processed_image, alpha=1.3, beta=20
                )
            elif quality_check["is_too_bright"]:
                logger.info("Attempting to darken bright image")
                processed_image = cv2.convertScaleAbs(
                    processed_image, alpha=0.8, beta=-10
                )

        # Always proceed with detection for real biometric mode
        logger.info("Proceeding with face detection on processed image")

        # Detect faces on processed image
        logger.info("Detecting faces in processed image")
        face_locations, face_landmarks = self.detect_faces(processed_image)
        logger.info(f"Face detection result: {len(face_locations)} faces found")

        if not face_locations:
            logger.error("No face detected in image")
            return {
                "success": False,
                "error": "No face detected",
                "quality_check": quality_check,
            }

        if len(face_locations) > 1:
            logger.warning(
                f"Multiple faces detected: {len(face_locations)}, selecting largest face"
            )

            # Find the largest face by area
            largest_face_idx = 0
            largest_area = 0

            for i, (top, right, bottom, left) in enumerate(face_locations):
                face_width = right - left
                face_height = bottom - top
                face_area = face_width * face_height
                logger.info(
                    f"Face {i+1}: size {face_width}x{face_height}, area: {face_area}"
                )

                if face_area > largest_area:
                    largest_area = face_area
                    largest_face_idx = i

            logger.info(
                f"Selected largest face (index {largest_face_idx}) with area: {largest_area}"
            )
            face_locations = [face_locations[largest_face_idx]]

            # Also filter face_landmarks if available
            if face_landmarks and largest_face_idx < len(face_landmarks):
                face_landmarks = [face_landmarks[largest_face_idx]]
            else:
                logger.warning("Face landmarks not available for selected face")
                face_landmarks = []

        # Extract face encoding and check eye visibility
        logger.info("Extracting face encoding")
        face_location = face_locations[0]
        first_face_landmarks = face_landmarks[0] if face_landmarks else None
        encoding, has_eyes = self.extract_face_encoding(
            image, face_location, first_face_landmarks
        )

        if encoding is None:
            logger.exception("Failed to extract face encoding")
            return {"success": False, "error": "Failed to extract face encoding"}

        logger.info(f"Face encoding extracted successfully: shape={encoding.shape}")
        logger.info(f"üëÅÔ∏è Final eye visibility result: {has_eyes}")

        # Calculate face size ratio
        top, right, bottom, left = face_location
        face_width = right - left
        face_height = bottom - top
        image_height, image_width = image.shape[:2]
        face_size_ratio = (face_width * face_height) / (image_width * image_height)

        processing_time = int((time.time() - start_time) * 1000)

        return {
            "success": True,
            "encoding": encoding.tolist(),  # Convert to list for JSON serialization
            "quality_check": quality_check,
            "face_location": face_location,
            "face_size_ratio": face_size_ratio,
            "has_eyes": has_eyes,
            "processing_time_ms": processing_time,
        }

    def process_multiple_images(self, base64_images: List[str]) -> Dict[str, Any]:
        """
        Process multiple images for registration

        Args:
            base64_images: List of base64 encoded images

        Returns:
            Dictionary with processing results
        """
        results = []
        successful_encodings = []

        for idx, base64_image in enumerate(base64_images):
            result = self.process_registration_image(base64_image)
            results.append(result)

            if result["success"]:
                successful_encodings.append(
                    {
                        "vector": result["encoding"],
                        "quality_score": result["quality_check"]["quality_score"],
                        "created_at": np.datetime64("now").tolist(),
                        "angle": f"angle_{idx}",  # You can improve this with actual angle detection
                    }
                )

        return {
            "success": len(successful_encodings) > 0,
            "encodings": successful_encodings,
            "processed_count": len(base64_images),
            "successful_count": len(successful_encodings),
            "results": results,
        }

    def process_images(self, base64_images: List[str]) -> Dict[str, Any]:
        """
        Process images for registration (alias for process_multiple_images)

        Args:
            base64_images: List of base64 encoded images

        Returns:
            Dictionary with processing results
        """
        return self.process_multiple_images(base64_images)

    def compare_faces(
        self, unknown_encoding: np.ndarray, known_encodings: List[np.ndarray]
    ) -> Tuple[bool, float]:
        """
        Compare an unknown face encoding with known encodings

        Args:
            unknown_encoding: Encoding of the face to check
            known_encodings: List of known face encodings

        Returns:
            Tuple of (match_found, best_distance)
        """
        if not known_encodings:
            return False, 1.0

        try:
            # Calculate face distances
            distances = face_recognition.face_distance(
                known_encodings, unknown_encoding
            )

            # Find the best match
            best_distance = min(distances)
            best_match_index = np.argmin(distances)

            # Check if it's a match based on tolerance
            is_match = best_distance <= self.tolerance

            # Convert distance to confidence score (0-1, where 1 is perfect match)
            confidence = 1 - best_distance

            return is_match, float(confidence)

        except Exception as e:
            from core.logging_utils import err_tag
            logger.error("Failed to compare faces", extra={"err": err_tag(e)})
            return False, 0.0

    def find_matching_employee(
        self, base64_image: str, all_embeddings: List[Tuple[int, List[Dict]]]
    ) -> Dict[str, Any]:
        """
        Find matching employee from a face image

        Args:
            base64_image: Base64 encoded image
            all_embeddings: List of tuples (employee_id, embeddings)

        Returns:
            Dictionary with matching results
        """
        start_time = time.time()

        # Process the input image
        result = self.process_registration_image(base64_image)
        if not result["success"]:
            return {
                "success": False,
                "error": result.get("error", "Failed to process image"),
                "details": result,
            }

        unknown_encoding = np.array(result["encoding"])

        # Compare with all known embeddings
        best_match_employee_id = None
        best_confidence = 0.0
        all_matches = []  # Track all matches for debugging

        logger.info(
            f"üîç Face matching debug - comparing against {len(all_embeddings)} employees"
        )

        for employee_id, employee_embeddings in all_embeddings:
            # Extract encoding vectors
            known_encodings = []
            for embedding in employee_embeddings:
                if "vector" in embedding:
                    known_encodings.append(np.array(embedding["vector"]))

            if not known_encodings:
                logger.debug(f"   - Employee {employee_id}: No encodings found")
                continue

            # Compare with this employee's encodings
            is_match, confidence = self.compare_faces(unknown_encoding, known_encodings)

            logger.info(
                f"   - Employee {employee_id}: confidence={confidence:.3f}, match={is_match}"
            )
            all_matches.append((employee_id, confidence, is_match))

            if is_match and confidence > best_confidence:
                best_match_employee_id = employee_id
                best_confidence = confidence

        # Log all results sorted by confidence
        all_matches.sort(key=lambda x: x[1], reverse=True)
        logger.info(f"üéØ All matching results (sorted by confidence):")
        for emp_id, conf, match in all_matches[:5]:  # Top 5 results
            logger.info(f"   - Employee {emp_id}: {conf:.3f} {'‚úÖ' if match else '‚ùå'}")

        logger.info(
            f"üèÜ Best match: Employee {best_match_employee_id} with confidence {best_confidence:.3f}"
        )

        processing_time = int((time.time() - start_time) * 1000)

        if best_match_employee_id:
            return {
                "success": True,
                "employee_id": best_match_employee_id,
                "confidence": best_confidence,
                "processing_time_ms": processing_time,
                "quality_check": result["quality_check"],
            }
        else:
            return {
                "success": False,
                "error": "No matching employee found",
                "processing_time_ms": processing_time,
                "quality_check": result["quality_check"],
            }


# Global instance
face_processor = FaceProcessor()
